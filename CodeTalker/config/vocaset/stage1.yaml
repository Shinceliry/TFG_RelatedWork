DATA:
  dataset: vocaset
  # データルートには、以下のように .wav と .npy が並んだディレクトリ構造を置いてください
  #  ┣── wav/    ... 音声ファイルたち（16kHz）
  #  ┗── vertices_npy/ ... 対応する頂点の座標時系列(npy)
  data_root: ./vocaset/
  wav_path: wav
  vertices_path: vertices_npy

  # ↓↓↓ ここを templates.pkl から FLAME_sample.ply に置き換える
  #      FLAME_sample.ply は全サンプルで使う単一メッシュとして扱う
  template_file: FLAME_sample.ply

  # VQ-VAE学習時は音声不要なら Falseに
  read_audio: False

  # 今回は単一スタイル(S0)だけにする例。val/testには何も割り当てずランダム分割。
  train_subjects: S0
  val_subjects: 
  test_subjects: 

LOSS:
  quant_loss_weight: 1.0

NETWORK:
  arch: stage1_vocaset
  in_dim: 15069     # 頂点数×3 (FLAMEの場合 5023頂点×3=15069) に合わせる
  hidden_size: 1024
  num_hidden_layers: 6
  num_attention_heads: 8
  intermediate_size: 1536
  window_size: 1
  quant_factor: 0
  face_quan_num: 16
  neg: 0.2
  INaffine: False

VQuantizer:
  n_embed: 256
  zquant_dim: 64

TRAIN:
  use_sgd: False
  sync_bn: False
  train_gpu: [0]
  workers: 4
  batch_size: 1
  batch_size_val: 1
  base_lr: 0.0001
  StepLR: True
  warmup_steps: 1
  adaptive_lr: False
  factor: 0.3
  patience: 3
  threshold: 0.0001
  poly_lr: False
  epochs: 200
  step_size: 20
  gamma: 0.5
  start_epoch: 0
  power: 0.9
  momentum: 0.9
  weight_decay: 0.002
  manual_seed: 131
  print_freq: 10
  save_freq: 1
  save_path:
  weight:
  resume:
  evaluate: True
  eval_freq: 10

Distributed:
  dist_url: tcp://127.0.0.1:6701
  dist_backend: 'nccl'
  multiprocessing_distributed: False
  world_size: 1
  rank: 0

TEST:
  test_workers: 0
  test_gpu: [0]
  test_batch_size: 1
  save: True
  model_path:
  save_folder: